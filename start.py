import os
from dotenv import load_dotenv
from telegram import Update, InlineKeyboardMarkup, InlineKeyboardButton
from telegram.ext import (
    ApplicationBuilder,
    MessageHandler,
    CommandHandler,
    CallbackQueryHandler,
    filters,
    ContextTypes
)
import ffmpeg
from groq import Groq
import requests
from edge_tts import Communicate
import shutil

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è ffmpeg
if shutil.which("ffmpeg") is None:
    raise RuntimeError("ffmpeg –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ Dockerfile –∏–ª–∏ –ª–æ–≥–∏.")

load_dotenv()
groq_api_key = os.getenv("GROQ_API_KEY")
telegram_token = os.getenv("TELEGRAM_TOKEN")
groq_client = Groq(api_key=groq_api_key)

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞ (user_id -> list of (prompt, answer))
chat_history = {}

def convert_ogg_to_mp3(in_file, out_file):
    (
        ffmpeg
        .input(in_file)
        .output(out_file, format='mp3', acodec='libmp3lame')
        .run(overwrite_output=True, quiet=True)
    )

def transcribe_whisper_groq(audio_path, fallback_models=["whisper-large-v3", "whisper-large-v3-turbo"]):
    for model in fallback_models:
        with open(audio_path, 'rb') as f:
            files = {'file': f}
            url = "https://api.groq.com/openai/v1/audio/transcriptions"
            headers = {"Authorization": f"Bearer {groq_api_key}"}
            data = {
                "model": model,
                "language": "ru",
                "response_format": "text"
            }
            try:
                response = requests.post(url, files=files, headers=headers, data=data, timeout=60)
                if response.status_code == 200 and response.text.strip():
                    return response.text.strip()
            except Exception as e:
                print(f"Groq Whisper {model} failed:", e)
    return ""

async def synthesize_voice(text, filename="answer.mp3", lang="ru-RU", voice="ru-RU-DmitryNeural"):
    communicate = Communicate(text, voice=voice)
    await communicate.save(filename)

async def start_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    keyboard = [
        [
            InlineKeyboardButton("‚ñ∂Ô∏è –°—Ç–∞—Ä—Ç", callback_data="start"),
            InlineKeyboardButton("üìù –ò—Å–ø—Ä–∞–≤–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é", callback_data="fix_transcript"),
        ],
        [
            InlineKeyboardButton("üîä –û–∑–≤—É—á–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ", callback_data="voice_fixed"),
        ],
        [
            InlineKeyboardButton("üìú –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞", callback_data="history"),
            InlineKeyboardButton("‚ùì –ü–æ–º–æ—â—å", callback_data="help"),
        ]
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)
    text = (
        "üëã *–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å!*\n\n"
        "–û—Ç–ø—Ä–∞–≤—å—Ç–µ –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ ‚Äî –±–æ—Ç —Å—Ä–∞–∑—É –ø–æ–∫–∞–∂–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é, –æ—Ç–≤–µ—Ç–∏—Ç —Ç–µ–∫—Å—Ç–æ–º –∏ –≥–æ–ª–æ—Å–æ–º.\n\n"
        "–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ –∏–∑ –º–µ–Ω—é:"
    )
    await update.message.reply_text(text, reply_markup=reply_markup, parse_mode="Markdown")

async def button_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    user_id = query.from_user.id

    if query.data == "start":
        await query.message.reply_text(
            "‚ñ∂Ô∏è –í—ã –º–æ–∂–µ—Ç–µ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –Ω–æ–≤–æ–µ –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ!"
        )
    elif query.data == "history":
        history = chat_history.get(user_id, [])
        if not history:
            await query.message.reply_text("üìú –ò—Å—Ç–æ—Ä–∏—è –ø—É—Å—Ç–∞!")
        else:
            text = "\n\n".join([f"üë§ *–í—ã:* {h[0]}\nü§ñ *–ë–æ—Ç:* {h[1]}" for h in history])
            await query.message.reply_text(f"–í–∞—à–∞ –∏—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞:\n\n{text}", parse_mode="Markdown")
    elif query.data == "fix_transcript":
        context.user_data["fix_mode"] = True
        await query.message.reply_text("üìù –í–≤–µ–¥–∏—Ç–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏:")
    elif query.data == "voice_fixed":
        fixed = context.user_data.get("fixed_transcript")
        if not fixed:
            await query.message.reply_text("üîñ –ù–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏. –í–≤–µ–¥–∏—Ç–µ –µ—ë —á–µ—Ä–µ–∑ 'üìù –ò—Å–ø—Ä–∞–≤–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é'.")
        else:
            await synthesize_voice(fixed, filename="fixed.mp3", lang="ru-RU", voice="ru-RU-DmitryNeural")
            with open("fixed.mp3", "rb") as f:
                await query.message.reply_voice(voice=f)
            await query.message.reply_text("üîä –ì–æ—Ç–æ–≤–æ, –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –æ–∑–≤—É—á–µ–Ω!")
    elif query.data == "help":
        help_text = (
            "‚ùì *–ß—Ç–æ —É–º–µ–µ—Ç –±–æ—Ç:*\n\n"
            "- –ü–æ–ª—É—á–∞—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–µ –∏ –≤—ã–≤–æ–¥–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é\n"
            "- –û—Ç–≤–µ—á–∞—Ç—å GPT –ø—Ä—è–º–æ –≤ —á–∞—Ç–µ\n"
            "- –û–∑–≤—É—á–∏–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –∏ –≤–∞—à–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏\n"
            "- –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –≤—Å—é –≤–∞—à—É –∏—Å—Ç–æ—Ä–∏—é –æ–±—â–µ–Ω–∏—è\n"
            "- –î–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –≤–æ—Å–ø–æ–ª—å–∑—É–π—Ç–µ—Å—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –∫–Ω–æ–ø–∫–æ–π!"
        )
        await query.message.reply_text(help_text, parse_mode="Markdown")


async def button_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    user_id = query.from_user.id

    if query.data == "start":
        await query.message.reply_text("–ü—Ä–∏–≤–µ—Ç! –û—Ç–ø—Ä–∞–≤—å—Ç–µ –≥–æ–ª–æ—Å–æ–≤–æ–µ, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –∏ –æ—Ç–≤–µ—Ç.")
    elif query.data == "history":
        history = chat_history.get(user_id, [])
        if not history:
            await query.message.reply_text("–ò—Å—Ç–æ—Ä–∏—è –ø—É—Å—Ç–∞!")
        else:
            text = "\n---\n".join([f"–í—ã: {h[0]}\n–ë–æ—Ç: {h[1]}" for h in history])
            await query.message.reply_text(f"–í–∞—à–∞ –∏—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞:\n{text}")
    elif query.data == "fix_transcript":
        context.user_data["fix_mode"] = True
        await query.message.reply_text("–í–≤–µ–¥–∏—Ç–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é —Ç–µ–∫—Å—Ç–æ–º:")
    elif query.data == "voice_fixed":
        fixed = context.user_data.get("fixed_transcript")
        if not fixed:
            await query.message.reply_text("–ù–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏. –í–≤–µ–¥–∏—Ç–µ –µ—ë —á–µ—Ä–µ–∑ '–ò—Å–ø—Ä–∞–≤–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é'.")
        else:
            await synthesize_voice(fixed, filename="fixed.mp3", lang="ru-RU", voice="ru-RU-DmitryNeural")
            with open("fixed.mp3", "rb") as f:
                await query.message.reply_voice(voice=f)

async def voice_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    voice = await update.message.voice.get_file()
    await voice.download_to_drive("voice.ogg")
    convert_ogg_to_mp3("voice.ogg", "voice.mp3")
    audio_path = "voice.mp3"

    prompt = transcribe_whisper_groq(audio_path)
    if not prompt:
        await update.message.reply_text("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ.")
        return

    await update.message.reply_text(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è:\n{prompt}")

    response = groq_client.chat.completions.create(
        model="openai/gpt-oss-120b",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.12,
        max_tokens=512
    )
    answer_text = response.choices[0].message.content

    await update.message.reply_text(answer_text)
    await synthesize_voice(answer_text, filename="answer.mp3", lang="ru-RU", voice="ru-RU-DmitryNeural")
    with open("answer.mp3", "rb") as f:
        await update.message.reply_voice(voice=f)

    chat_history.setdefault(user_id, []).append((prompt, answer_text))
    context.user_data["last_transcript"] = prompt

async def text_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    text = update.message.text

    if context.user_data.get("fix_mode"):
        context.user_data["fixed_transcript"] = text
        context.user_data["fix_mode"] = False
        await update.message.reply_text(f"–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {text}")
        return

    response = groq_client.chat.completions.create(
        model="openai/gpt-oss-120b",
        messages=[{"role": "user", "content": text}],
        temperature=0.12,
        max_tokens=512
    )
    answer_text = response.choices[0].message.content
    await update.message.reply_text(answer_text)
    await synthesize_voice(answer_text, filename="answer.mp3", lang="ru-RU", voice="ru-RU-DmitryNeural")
    with open("answer.mp3", "rb") as f:
        await update.message.reply_voice(voice=f)

    chat_history.setdefault(user_id, []).append((text, answer_text))

if __name__ == "__main__":
    app = ApplicationBuilder().token(telegram_token).build()
    app.add_handler(CommandHandler("start", start_handler))
    app.add_handler(CallbackQueryHandler(button_handler))
    app.add_handler(MessageHandler(filters.VOICE, voice_handler))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, text_handler))
    app.run_polling()
