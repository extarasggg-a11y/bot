import os
from dotenv import load_dotenv
from telegram import (
    Update,
    InlineKeyboardMarkup,
    InlineKeyboardButton,
    ReplyKeyboardMarkup
)
from telegram.ext import (
    ApplicationBuilder,
    MessageHandler,
    CommandHandler,
    CallbackQueryHandler,
    filters,
    ContextTypes
)
import ffmpeg
from groq import Groq
import requests
from edge_tts import Communicate
import shutil

if shutil.which("ffmpeg") is None:
    raise RuntimeError("ffmpeg –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ Dockerfile –∏–ª–∏ –ª–æ–≥–∏.")

load_dotenv()
groq_api_key = os.getenv("GROQ_API_KEY")
telegram_token = os.getenv("TELEGRAM_TOKEN")
groq_client = Groq(api_key=groq_api_key)
chat_history = {}

def convert_ogg_to_mp3(in_file, out_file):
    (
        ffmpeg
        .input(in_file)
        .output(out_file, format='mp3', acodec='libmp3lame')
        .run(overwrite_output=True, quiet=True)
    )

def transcribe_whisper_groq(audio_path, fallback_models=None):
    if fallback_models is None:
        fallback_models = [
            "whisper-large-v3",
            "whisper-large-v3-turbo",
            "whisper-medium",
            "whisper-medium-turbo",
        ]
    for model in fallback_models:
        with open(audio_path, 'rb') as f:
            files = {'file': f}
            url = "https://api.groq.com/openai/v1/audio/transcriptions"
            headers = {"Authorization": f"Bearer {groq_api_key}"}
            data = {
                "model": model,
                "language": "ru",
                "response_format": "text"
            }
            try:
                response = requests.post(url, files=files, headers=headers, data=data, timeout=60)
                text = response.text.strip()
                text = text.replace('\n', ' ').replace('\r', '').replace('\t', ' ').strip()
                if response.status_code == 200 and text and len(text) > 4:
                    return text
            except Exception as e:
                print(f"Groq Whisper {model} failed:", e)
    return ""

async def synthesize_voice(text, filename="voice.mp3", lang="ru-RU", voice="ru-RU-DmitryNeural"):
    communicate = Communicate(text, voice=voice)
    await communicate.save(filename)

def get_reply_keyboard():
    keyboard = [["–ú–µ–Ω—é"]]
    return ReplyKeyboardMarkup(keyboard, resize_keyboard=True)

async def start_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    keyboard_inline = [
        [
            InlineKeyboardButton("‚ñ∂Ô∏è –°—Ç–∞—Ä—Ç", callback_data="start"),
            InlineKeyboardButton("üìù –ò—Å–ø—Ä–∞–≤–∏—Ç—å", callback_data="fix_transcript"),
        ],
        [
            InlineKeyboardButton("üîä –û–∑–≤—É—á–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ", callback_data="voice_fixed"),
        ],
        [
            InlineKeyboardButton("üóÇÔ∏è –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞", callback_data="history"),
            InlineKeyboardButton("‚ùì –ü–æ–º–æ—â—å", callback_data="help"),
        ]
    ]
    reply_markup_inline = InlineKeyboardMarkup(keyboard_inline)
    reply_markup_keyboard = get_reply_keyboard()
    text = (
        "üëã –î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å!\n\n"
        "üé§ –ü—Ä–∏—à–ª–∏—Ç–µ –≥–æ–ª–æ—Å–æ–≤–æ–µ ‚Äî –±–æ—Ç –ø–æ–∫–∞–∂–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é, –æ—Ç–≤–µ—Ç–∏—Ç –∏ –æ–∑–≤—É—á–∏—Ç –µ–≥–æ.\n"
        "–ú–µ–Ω—é –Ω–∏–∂–µ ‚¨áÔ∏è"
    )
    await update.message.reply_text(text, reply_markup=reply_markup_inline)
    await update.message.reply_text(
        "–î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –≤—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–Ω–æ–ø–∫—É ¬´–ú–µ–Ω—é¬ª ‚¨áÔ∏è –ø–æ–¥ —Å—Ç—Ä–æ–∫–æ–π –≤–≤–æ–¥–∞.",
        reply_markup=reply_markup_keyboard
    )

async def menu_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await start_handler(update, context)

async def button_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    user_id = query.from_user.id
    reply_markup_keyboard = get_reply_keyboard()

    if query.data == "start":
        await query.message.reply_text("‚ñ∂Ô∏è –ì–æ—Ç–æ–≤ –∫ –Ω–æ–≤—ã–º —Å–æ–æ–±—â–µ–Ω–∏—è–º! –ü—Ä–æ—Å—Ç–æ –ø—Ä–∏—à–ª–∏—Ç–µ –≥–æ–ª–æ—Å–æ–≤–æ–µ üëá", reply_markup=reply_markup_keyboard)
    elif query.data == "history":
        history = chat_history.get(user_id, [])
        if not history:
            await query.message.reply_text("üóÇÔ∏è –ò—Å—Ç–æ—Ä–∏—è –ø—É—Å—Ç–∞! –ù–∞—á–Ω–∏—Ç–µ –¥–∏–∞–ª–æ–≥ ‚Äî –ø—Ä–∏—à–ª–∏—Ç–µ –≥–æ–ª–æ—Å –∏–ª–∏ —Ç–µ–∫—Å—Ç.", reply_markup=reply_markup_keyboard)
        else:
            blocks = []
            for h in history:
                part = f"üé§ {h['origin']}"
                if h.get("fixed"):
                    part += f"\n‚úèÔ∏è –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: {h['fixed']}"
                part += f"\nü§ñ –û—Ç–≤–µ—Ç: {h['answer']}"
                blocks.append(part)
            text = "\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n".join(blocks)
            await query.message.reply_text(f"üóÇÔ∏è –í–∞—à–∞ –∏—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞:\n\n{text}", reply_markup=reply_markup_keyboard)
    elif query.data == "fix_transcript":
        context.user_data["fix_mode"] = True
        await query.message.reply_text("üìù –í–≤–µ–¥–∏—Ç–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏:", reply_markup=reply_markup_keyboard)
    elif query.data == "voice_fixed":
        fixed = context.user_data.get("fixed_transcript")
        if not fixed:
            await query.message.reply_text("‚ùó –ù–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏. –í–≤–µ–¥–∏—Ç–µ –µ—ë —á–µ—Ä–µ–∑ –∫–Ω–æ–ø–∫—É 'üìù –ò—Å–ø—Ä–∞–≤–∏—Ç—å'.", reply_markup=reply_markup_keyboard)
        else:
            await synthesize_voice(fixed, filename="fixed.mp3", lang="ru-RU", voice="ru-RU-DmitryNeural")
            with open("fixed.mp3", "rb") as f:
                await query.message.reply_voice(voice=f)
            await query.message.reply_text("üîä –í–∞—à–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –æ–∑–≤—É—á–µ–Ω–∞!", reply_markup=reply_markup_keyboard)
    elif query.data == "help":
        help_text = (
            "‚ùì –ß—Ç–æ —É–º–µ–µ—Ç –±–æ—Ç:\n"
            "‚Ä¢ –ì–æ–ª–æ—Å ‚Üí —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è + GPT-–æ—Ç–≤–µ—Ç + –æ–∑–≤—É—á–∫–∞\n"
            "‚Ä¢ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏ –æ–∑–≤—É—á–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏\n"
            "‚Ä¢ –í—Å—è –∏—Å—Ç–æ—Ä–∏—è –≤–∞—à–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏ –æ—Ç–≤–µ—Ç–æ–≤\n"
            "‚Ä¢ –ö—Ä–∞—Å–∏–≤—ã–µ –∏–∫–æ–Ω–∫–∏ –¥–ª—è –≤–∞—à–µ–≥–æ —É–¥–æ–±—Å—Ç–≤–∞\n\n"
            "–ü—Ä–∏—à–ª–∏—Ç–µ –≥–æ–ª–æ—Å–æ–≤–æ–µ –∏–ª–∏ —Ç–µ–∫—Å—Ç ‚Äî –ø–æ–ª—É—á–∏—Ç–µ —Å—Ä–∞–∑—É –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç!"
        )
        await query.message.reply_text(help_text, reply_markup=reply_markup_keyboard)

async def voice_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    voice = await update.message.voice.get_file()
    await voice.download_to_drive("voice.ogg")
    convert_ogg_to_mp3("voice.ogg", "voice.mp3")
    audio_path = "voice.mp3"

    prompt = transcribe_whisper_groq(audio_path)
    if not prompt or len(prompt) < 4:
        await update.message.reply_text(
            "‚ùó –ù–µ —É–¥–∞–ª–æ—Å—å —Ö–æ—Ä–æ—à–æ —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –≤–∞—à –≥–æ–ª–æ—Å. –í–æ—Å–ø–æ–ª—å–∑—É–π—Ç–µ—Å—å –∫–Ω–æ–ø–∫–æ–π 'üìù –ò—Å–ø—Ä–∞–≤–∏—Ç—å', —á—Ç–æ–±—ã –≤—Ä—É—á–Ω—É—é –≤–≤–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç!",
            reply_markup=get_reply_keyboard()
        )
        context.user_data["fix_mode"] = True
        return

    await update.message.reply_text(f"üé§ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è:\n{prompt}", reply_markup=get_reply_keyboard())
    context.user_data["last_transcript"] = prompt
    context.user_data["fixed_transcript"] = None

    response = groq_client.chat.completions.create(
        model="openai/gpt-oss-120b",
        messages=[
            {"role": "system", "content": "–¢—ã –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫, –≤—Å–µ–≥–¥–∞ –æ—Ç–≤–µ—á–∞–π –æ–±—ã—á–Ω—ã–º —Ä—É—Å—Å–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º, –±–µ–∑ Markdown, –±–µ–∑ ##, –±–µ–∑ **. –î–æ–±–∞–≤–ª—è–π —É–º–µ—Å—Ç–Ω—ã–µ —ç–º–æ–¥–∑–∏ –¥–ª—è –∫—Ä–∞—Å–æ—Ç—ã –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä: ü§ñ, ‚úèÔ∏è, üì¶, üìù, üé§, üîä, üí°, ‚õëÔ∏è, üóÇÔ∏è, üëç)"},
            {"role": "user", "content": prompt}
        ],
        temperature=0.12,
        max_tokens=512
    )
    answer_text = response.choices[0].message.content

    await update.message.reply_text(f"ü§ñ –û—Ç–≤–µ—Ç:\n{answer_text}", reply_markup=get_reply_keyboard())
    await synthesize_voice(answer_text, filename="answer.mp3", lang="ru-RU", voice="ru-RU-DmitryNeural")
    with open("answer.mp3", "rb") as f:
        await update.message.reply_voice(voice=f)

    chat_history.setdefault(user_id, []).append({"origin": prompt, "fixed": None, "answer": answer_text})

async def text_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    text = update.message.text

    if context.user_data.get("fix_mode"):
        context.user_data["fixed_transcript"] = text
        context.user_data["fix_mode"] = False
        await update.message.reply_text(f"‚úèÔ∏è –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {text}", reply_markup=get_reply_keyboard())

        response = groq_client.chat.completions.create(
            model="openai/gpt-oss-120b",
            messages=[
                {"role": "system", "content": "–¢—ã –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫, –≤—Å–µ–≥–¥–∞ –æ—Ç–≤–µ—á–∞–π –æ–±—ã—á–Ω—ã–º —Ä—É—Å—Å–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º, –±–µ–∑ Markdown, –±–µ–∑ ##, –±–µ–∑ **. –î–æ–±–∞–≤–ª—è–π —É–º–µ—Å—Ç–Ω—ã–µ —ç–º–æ–¥–∑–∏ –¥–ª—è –∫—Ä–∞—Å–æ—Ç—ã –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä: ü§ñ, ‚úèÔ∏è, üì¶, üìù, üé§, üîä, üí°, ‚õëÔ∏è, üóÇÔ∏è, üëç)"},
                {"role": "user", "content": text}
            ],
            temperature=0.12,
            max_tokens=512
        )
        answer_text = response.choices[0].message.content
        await update.message.reply_text(f"ü§ñ –û—Ç–≤–µ—Ç:\n{answer_text}", reply_markup=get_reply_keyboard())
        await synthesize_voice(answer_text, filename="answer.mp3", lang="ru-RU", voice="ru-RU-DmitryNeural")
        with open("answer.mp3", "rb") as f:
            await update.message.reply_voice(voice=f)

        chat_history.setdefault(user_id, []).append({
            "origin": context.user_data.get("last_transcript", ""),
            "fixed": text,
            "answer": answer_text
        })
        return

    response = groq_client.chat.completions.create(
        model="openai/gpt-oss-120b",
        messages=[
            {"role": "system", "content": "–¢—ã –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫, –≤—Å–µ–≥–¥–∞ –æ—Ç–≤–µ—á–∞–π –æ–±—ã—á–Ω—ã–º —Ä—É—Å—Å–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º, –±–µ–∑ Markdown, –±–µ–∑ ##, –±–µ–∑ **. –î–æ–±–∞–≤–ª—è–π —É–º–µ—Å—Ç–Ω—ã–µ —ç–º–æ–¥–∑–∏ –¥–ª—è –∫—Ä–∞—Å–æ—Ç—ã –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä: ü§ñ, ‚úèÔ∏è, üì¶, üìù, üé§, üîä, üí°, ‚õëÔ∏è, üóÇÔ∏è, üëç)"},
            {"role": "user", "content": text}
        ],
        temperature=0.12,
        max_tokens=512
    )
    answer_text = response.choices[0].message.content
    await update.message.reply_text(f"ü§ñ –û—Ç–≤–µ—Ç:\n{answer_text}", reply_markup=get_reply_keyboard())
    await synthesize_voice(answer_text, filename="answer.mp3", lang="ru-RU", voice="ru-RU-DmitryNeural")
    with open("answer.mp3", "rb") as f:
        await update.message.reply_voice(voice=f)

    chat_history.setdefault(user_id, []).append({"origin": text, "fixed": None, "answer": answer_text})

if __name__ == "__main__":
    app = ApplicationBuilder().token(telegram_token).build()
    app.add_handler(CommandHandler("start", start_handler))
    app.add_handler(MessageHandler(filters.TEXT & filters.Regex("^–ú–µ–Ω—é$"), menu_handler))
    app.add_handler(CallbackQueryHandler(button_handler))
    app.add_handler(MessageHandler(filters.VOICE, voice_handler))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND & ~filters.Regex("^–ú–µ–Ω—é$"), text_handler))
    app.run_polling()
