import os
from dotenv import load_dotenv
from telegram import Update, InlineKeyboardMarkup, InlineKeyboardButton
from telegram.ext import (
    ApplicationBuilder,
    MessageHandler,
    CommandHandler,
    CallbackQueryHandler,
    filters,
    ContextTypes
)
import ffmpeg
from groq import Groq
import requests
from edge_tts import Communicate
import shutil

if shutil.which("ffmpeg") is None:
    raise RuntimeError("ffmpeg –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ Dockerfile –∏–ª–∏ –ª–æ–≥–∏.")

load_dotenv()
groq_api_key = os.getenv("GROQ_API_KEY")
telegram_token = os.getenv("TELEGRAM_TOKEN")
groq_client = Groq(api_key=groq_api_key)

# user_id -> list of dicts: {"origin": original, "fixed": fixed (–∏–ª–∏ None), "answer": answer}
chat_history = {}

def convert_ogg_to_mp3(in_file, out_file):
    (
        ffmpeg
        .input(in_file)
        .output(out_file, format='mp3', acodec='libmp3lame')
        .run(overwrite_output=True, quiet=True)
    )

def transcribe_whisper_groq(audio_path, fallback_models=None):
    if fallback_models is None:
        fallback_models = [
            "whisper-large-v3",
            "whisper-large-v3-turbo",
            "whisper-medium",
            "whisper-medium-turbo",
        ]
    for model in fallback_models:
        with open(audio_path, 'rb') as f:
            files = {'file': f}
            url = "https://api.groq.com/openai/v1/audio/transcriptions"
            headers = {"Authorization": f"Bearer {groq_api_key}"}
            data = {
                "model": model,
                "language": "ru",
                "response_format": "text"
            }
            try:
                response = requests.post(url, files=files, headers=headers, data=data, timeout=60)
                text = response.text.strip()
                text = text.replace('\n', ' ').replace('\r', '').replace('\t', ' ').strip()
                if response.status_code == 200 and text and len(text) > 4:
                    return text
            except Exception as e:
                print(f"Groq Whisper {model} failed:", e)
    return ""

async def synthesize_voice(text, filename="voice.mp3", lang="ru-RU", voice="ru-RU-DmitryNeural"):
    communicate = Communicate(text, voice=voice)
    await communicate.save(filename)

async def start_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    keyboard = [
        [
            InlineKeyboardButton("‚ñ∂Ô∏è –°—Ç–∞—Ä—Ç", callback_data="start"),
            InlineKeyboardButton("üìù –ò—Å–ø—Ä–∞–≤–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é", callback_data="fix_transcript"),
        ],
        [
            InlineKeyboardButton("üîä –û–∑–≤—É—á–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ", callback_data="voice_fixed"),
        ],
        [
            InlineKeyboardButton("üìú –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞", callback_data="history"),
            InlineKeyboardButton("‚ùì –ü–æ–º–æ—â—å", callback_data="help"),
        ]
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)
    text = (
        "üëã *–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å!*\n\n"
        "–û—Ç–ø—Ä–∞–≤—å—Ç–µ –≥–æ–ª–æ—Å–æ–≤–æ–µ ‚Äî –±–æ—Ç –ø–æ–∫–∞–∂–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é, –æ—Ç–≤–µ—Ç–∏—Ç GPT –∏ –æ–∑–≤—É—á–∏—Ç –µ–≥–æ.\n"
        "–ò—Å–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç ‚Äî –∫–Ω–æ–ø–∫—É ¬´üìù –ò—Å–ø—Ä–∞–≤–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é¬ª.\n"
        "–û–∑–≤—É—á—å—Ç–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ ‚Äî ¬´üîä –û–∑–≤—É—á–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ¬ª.\n"
        "–ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –∏—Å—Ç–æ—Ä–∏—é ‚Äî ¬´üìú –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞¬ª.\n"
        "–î–ª—è —Å–ø—Ä–∞–≤–∫–∏ ‚Äî ¬´‚ùì –ü–æ–º–æ—â—å¬ª.\n"
        "–ú–µ–Ω—é:"
    )
    await update.message.reply_text(text, reply_markup=reply_markup, parse_mode="Markdown")

async def button_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    user_id = query.from_user.id

    if query.data == "start":
        await query.message.reply_text("‚ñ∂Ô∏è –ü—Ä–∏—à–ª–∏—Ç–µ –Ω–æ–≤–æ–µ –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ!")
    elif query.data == "history":
        history = chat_history.get(user_id, [])
        if not history:
            await query.message.reply_text("üìú –ò—Å—Ç–æ—Ä–∏—è –ø—É—Å—Ç–∞!")
        else:
            blocks = []
            for h in history:
                part = (
                    f"üë§ *–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è:* {h['origin']}"
                )
                if h.get("fixed"):
                    part += f"\n‚úèÔ∏è *–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ:* {h['fixed']}"
                part += f"\nü§ñ *–û—Ç–≤–µ—Ç:* {h['answer']}"
                blocks.append(part)
            text = "\n\n".join(blocks)
            await query.message.reply_text(f"–í–∞—à–∞ –∏—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞:\n\n{text}", parse_mode="Markdown")
    elif query.data == "fix_transcript":
        context.user_data["fix_mode"] = True
        await query.message.reply_text("üìù –í–≤–µ–¥–∏—Ç–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏:")
    elif query.data == "voice_fixed":
        fixed = context.user_data.get("fixed_transcript")
        if not fixed:
            await query.message.reply_text("üîñ –ù–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏. –í–≤–µ–¥–∏—Ç–µ –µ—ë —á–µ—Ä–µ–∑ ¬´üìù –ò—Å–ø—Ä–∞–≤–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é¬ª.")
        else:
            await synthesize_voice(fixed, filename="fixed.mp3", lang="ru-RU", voice="ru-RU-DmitryNeural")
            with open("fixed.mp3", "rb") as f:
                await query.message.reply_voice(voice=f)
            await query.message.reply_text("üîä –û–∑–≤—É—á–µ–Ω–∞ –≤–∞—à–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è!")
    elif query.data == "help":
        help_text = (
            "‚ùì *–ß—Ç–æ —É–º–µ–µ—Ç –±–æ—Ç:*\n\n"
            "- –ü–æ–ª—É—á–∞—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–µ, –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é (–∏ –∏—Å–ø—Ä–∞–≤–ª—è—Ç—å –≤—Ä—É—á–Ω—É—é)\n"
            "- –û—Ç–≤–µ—á–∞—Ç—å GPT –Ω–∞ —Ä—É—Å—Å–∫–æ–º –≤ —á–∞—Ç\n"
            "- –û–∑–≤—É—á–∏–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –∏ –≤–∞—à–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã\n"
            "- –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π –∏ –æ—Ç–≤–µ—Ç–æ–≤\n\n"
            "–ú–µ–Ω—é –¥–æ—Å—Ç—É–ø–Ω–æ –≤—Å–µ–≥–¥–∞ —á–µ—Ä–µ–∑ /start"
        )
        await query.message.reply_text(help_text, parse_mode="Markdown")

async def voice_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    voice = await update.message.voice.get_file()
    await voice.download_to_drive("voice.ogg")
    convert_ogg_to_mp3("voice.ogg", "voice.mp3")
    audio_path = "voice.mp3"

    # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ
    prompt = transcribe_whisper_groq(audio_path)
    if not prompt or len(prompt) < 4:
        await update.message.reply_text(
            "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ.\n"
            "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑ –∏–ª–∏ –∏—Å–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ 'üìù –ò—Å–ø—Ä–∞–≤–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é'."
        )
        context.user_data["fix_mode"] = True
        return

    await update.message.reply_text(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è:\n{prompt}")
    context.user_data["last_transcript"] = prompt
    context.user_data["fixed_transcript"] = None  # –Ω–æ–≤—ã–π –≥–æ–ª–æ—Å ‚Äî —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ

    # GPT –≤—Å–µ–≥–¥–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º
    response = groq_client.chat.completions.create(
        model="openai/gpt-oss-120b",
        messages=[
            {"role": "system", "content": "–í—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫, –æ—Ç–≤–µ—á–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ù–µ –æ—Ç–≤–µ—á–∞–π—Ç–µ –Ω–∞ –¥—Ä—É–≥–∏—Ö —è–∑—ã–∫–∞—Ö."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.12,
        max_tokens=512
    )
    answer_text = response.choices[0].message.content

    await update.message.reply_text(answer_text)
    await synthesize_voice(answer_text, filename="answer.mp3", lang="ru-RU", voice="ru-RU-DmitryNeural")
    with open("answer.mp3", "rb") as f:
        await update.message.reply_voice(voice=f)

    # –ò—Å—Ç–æ—Ä–∏—è —Å —É—á–µ—Ç–æ–º —Ç–æ–ª—å–∫–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    chat_history.setdefault(user_id, []).append({"origin": prompt, "fixed": None, "answer": answer_text})

async def text_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.message.from_user.id
    text = update.message.text

    if context.user_data.get("fix_mode"):
        context.user_data["fixed_transcript"] = text
        context.user_data["fix_mode"] = False
        await update.message.reply_text(f"‚úèÔ∏è –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {text}")

        # GPT-–æ—Ç–≤–µ—Ç –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ ‚Äî –≤—Å–µ–≥–¥–∞ —Ä—É—Å—Å–∫–∏–π
        response = groq_client.chat.completions.create(
            model="openai/gpt-oss-120b",
            messages=[
                {"role": "system", "content": "–í—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫, –æ—Ç–≤–µ—á–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ù–µ –æ—Ç–≤–µ—á–∞–π—Ç–µ –Ω–∞ –¥—Ä—É–≥–∏—Ö —è–∑—ã–∫–∞—Ö."},
                {"role": "user", "content": text}
            ],
            temperature=0.12,
            max_tokens=512
        )
        answer_text = response.choices[0].message.content
        await update.message.reply_text(answer_text)
        await synthesize_voice(answer_text, filename="answer.mp3", lang="ru-RU", voice="ru-RU-DmitryNeural")
        with open("answer.mp3", "rb") as f:
            await update.message.reply_voice(voice=f)

        # –ò—Å—Ç–æ—Ä–∏—è ‚Äî –≤–∏–¥–Ω–æ, —á—Ç–æ —ç—Ç–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
        chat_history.setdefault(user_id, []).append({"origin": context.user_data.get("last_transcript", ""), "fixed": text, "answer": answer_text})
        return

    # –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
    response = groq_client.chat.completions.create(
        model="openai/gpt-oss-120b",
        messages=[
            {"role": "system", "content": "–í—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫, –æ—Ç–≤–µ—á–∞–π—Ç–µ —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ù–µ –æ—Ç–≤–µ—á–∞–π—Ç–µ –Ω–∞ –¥—Ä—É–≥–∏—Ö —è–∑—ã–∫–∞—Ö."},
            {"role": "user", "content": text}
        ],
        temperature=0.12,
        max_tokens=512
    )
    answer_text = response.choices[0].message.content
    await update.message.reply_text(answer_text)
    await synthesize_voice(answer_text, filename="answer.mp3", lang="ru-RU", voice="ru-RU-DmitryNeural")
    with open("answer.mp3", "rb") as f:
        await update.message.reply_voice(voice=f)

    chat_history.setdefault(user_id, []).append({"origin": text, "fixed": None, "answer": answer_text})

if __name__ == "__main__":
    app = ApplicationBuilder().token(telegram_token).build()
    app.add_handler(CommandHandler("start", start_handler))
    app.add_handler(CallbackQueryHandler(button_handler))
    app.add_handler(MessageHandler(filters.VOICE, voice_handler))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, text_handler))
    app.run_polling()
